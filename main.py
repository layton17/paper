import torch
import os
import time
import random
import numpy as np
import logging
import datetime
from torch.utils.data import DataLoader
from tqdm import tqdm

# å¼•å…¥è‡ªå®šä¹‰æ¨¡å—
from config import get_args_parser
from dataset import VideoDataset, collate_fn
from model import build_model
from text_encoder import CLIPTextEncoder, GloveTextEncoder
from matcher import HungarianMatcher
from criterion import SetCriterion
from engine import evaluate

# é…ç½®åŸºç¡€ Logger (æ§åˆ¶å°è¾“å‡º)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

# main.py

def train_one_epoch(model, criterion, data_loader, optimizer, device, epoch):
    model.train()
    criterion.train()
    
    total_loss = 0
    # [ç»Ÿè®¡å˜é‡åˆå§‹åŒ–]
    total_span_loss = 0
    total_giou_loss = 0
    total_quality_loss = 0
    total_saliency_loss = 0
    total_cont_loss = 0
    total_recfw_loss = 0   # [æ–°å¢] æ–‡æœ¬é‡æ„æŸå¤±ç»Ÿè®¡
    total_label_loss = 0   # [æ–°å¢] åˆ†ç±»æŸå¤±ç»Ÿè®¡
    
    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f"Epoch {epoch} Train")
    
    for i, batch in pbar:
        video_feat = batch['video_feat'].to(device)
        video_mask = batch['video_mask'].to(device)
        words_id = batch['words_id'].to(device)
        txt_ids = batch['txt_ids'].to(device)
        words_mask = batch['words_mask'].to(device)
        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in batch['targets']]

        # Forward
        outputs = model(video_feat, video_mask, words_id, words_mask, is_training=True)
        
        # Loss Calculation
        loss_dict = criterion(outputs, targets)
        weight_dict = criterion.weight_dict
        
        # è®¡ç®—åŠ æƒæ€» Loss
        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)

        # Backward
        optimizer.zero_grad()
        losses.backward()
        if hasattr(model.args, 'clip_max_norm') and model.args.clip_max_norm > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), model.args.clip_max_norm)
        optimizer.step()
        
        total_loss += losses.item()
        
        # [ä¿®æ”¹] æå–å„ä¸ªå­ Loss (ä½¿ç”¨ .get å®‰å…¨è·å–ï¼Œé˜²æ­¢æŸäº› loss æœªå¼€å¯æ—¶æŠ¥é”™)
        l_span = loss_dict.get('loss_span', torch.tensor(0.0)).item()
        l_giou = loss_dict.get('loss_giou', torch.tensor(0.0)).item()
        l_qual = loss_dict.get('loss_quality', torch.tensor(0.0)).item()
        l_cont = loss_dict.get('loss_contrastive', torch.tensor(0.0)).item()
        l_sal = loss_dict.get('loss_saliency', torch.tensor(0.0)).item()
        l_rec = loss_dict.get('loss_recfw', torch.tensor(0.0)).item()    # [æ–°å¢]
        l_label = loss_dict.get('loss_labels', torch.tensor(0.0)).item() # [æ–°å¢]
        
        # [ç´¯åŠ ç»Ÿè®¡]
        total_span_loss += l_span
        total_giou_loss += l_giou
        total_quality_loss += l_qual
        total_cont_loss += l_cont
        total_saliency_loss += l_sal
        total_recfw_loss += l_rec    # [æ–°å¢]
        total_label_loss += l_label  # [æ–°å¢]
        
        # [ä¿®æ”¹] å®æ—¶æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'L': f"{losses.item():.2f}",     # æ€» Weighted Loss
            'Cls': f"{l_label:.3f}",         # Label/Classification
            'Span': f"{l_span:.3f}",         # Span L1
            'GIoU': f"{l_giou:.3f}",
            'Qual': f"{l_qual:.3f}",
            'Sal': f"{l_sal:.2f}",           # Saliency (æ³¨æ„è¿™é‡Œä¿ç•™2ä½å°æ•°å› ä¸ºæ•°å€¼è¾ƒå¤§)
            'Cont': f"{l_cont:.3f}",         # Contrastive
            'Rec': f"{l_rec:.3f}"            # RecFW
        })
    
    # è®¡ç®— Epoch å¹³å‡å€¼
    avg_loss = total_loss / len(data_loader)
    avg_span = total_span_loss / len(data_loader)
    avg_giou = total_giou_loss / len(data_loader)
    avg_cont = total_cont_loss / len(data_loader)
    avg_sal = total_saliency_loss / len(data_loader)
    avg_rec = total_recfw_loss / len(data_loader)   # [æ–°å¢]
    avg_label = total_label_loss / len(data_loader) # [æ–°å¢]
    
    # [ä¿®æ”¹] æœ€ç»ˆæ—¥å¿—æ‰“å°æ‰€æœ‰ Loss
    logger.info(
        f"Epoch [{epoch}] Avg Loss: {avg_loss:.4f} | "
        f"Cls: {avg_label:.4f} | "
        f"Span: {avg_span:.4f} | "
        f"GIoU: {avg_giou:.4f} | "
        f"Sal: {avg_sal:.4f} | "
        f"Cont: {avg_cont:.4f} | "
        f"Rec: {avg_rec:.4f}"
    )
    
    return avg_loss

def main(args):
    device = torch.device(args.device)
    set_seed(args.seed)
    
    # 1. åˆ›å»º Checkpoint ä¿å­˜ç›®å½•
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)

    # æ—¥å¿—åˆ†ç¦»é€»è¾‘
    exp_name = os.path.basename(os.path.normpath(args.save_dir))
    log_dir = os.path.join("logs", exp_name)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(log_dir, f"train_{timestamp}.log")
    
    file_handler = logging.FileHandler(log_path, mode='w')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    
    root_logger = logging.getLogger('')
    for h in root_logger.handlers[:]:
        if isinstance(h, logging.FileHandler):
            root_logger.removeHandler(h)
    root_logger.addHandler(file_handler)
    
    logger.info(f"âœ… Log file created at: {log_path}")
    logger.info(f"âœ… Checkpoints will be saved to: {args.save_dir}")
    logger.info(f"Initializing Dataset: {args.dataset_name}")
    
    # -----------------------------------------------------------
    # 2. åŠ è½½æ•°æ®é›†
    # -----------------------------------------------------------
    dataset_train = VideoDataset(args, is_training=True)
    dataloader_train = DataLoader(
        dataset_train, 
        batch_size=args.batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=4,
        pin_memory=True
    )
    logger.info(f"Train dataset size: {len(dataset_train)}")

    test_anno_path = args.annotation_path.replace("train.txt", "test.txt")
    dataloader_val = None
    if os.path.exists(test_anno_path):
        logger.info(f"Loading Validation Dataset from: {test_anno_path}")
        args_val = type(args)(**vars(args)) 
        args_val.annotation_path = test_anno_path
        dataset_val = VideoDataset(args_val, is_training=False)
        
        if args.text_encoder_type == 'glove':
            dataset_val.word2idx = dataset_train.word2idx
            dataset_val.vocab = dataset_train.vocab
            
        dataloader_val = DataLoader(
            dataset_val, batch_size=args.batch_size, shuffle=False, 
            collate_fn=collate_fn, num_workers=4, pin_memory=True
        )
    else:
        logger.warning(f"Validation file not found at {test_anno_path}")

    # -----------------------------------------------------------
    # 3. åˆå§‹åŒ– Text Encoder
    # -----------------------------------------------------------
    text_encoder = None
    if args.text_encoder_type == 'clip':
        logger.info("Building CLIP Text Encoder...")
        text_encoder = CLIPTextEncoder(
            embed_dim=args.t_feat_dim,
            context_length=args.max_q_l,
            vocab_size=49408,
            transformer_width=512,
            transformer_heads=8,
            transformer_layers=12
        )
        
        if hasattr(args, 'clip_weight_path') and args.clip_weight_path:
            logger.info(f"Loading CLIP weights from {args.clip_weight_path}")
            try:
                loaded_obj = torch.load(args.clip_weight_path, map_location='cpu')
            except Exception:
                loaded_obj = torch.jit.load(args.clip_weight_path, map_location='cpu')
                
            if hasattr(loaded_obj, 'state_dict'):
                state_dict = loaded_obj.state_dict()
            else:
                state_dict = loaded_obj

            if 'positional_embedding' in state_dict:
                ckpt_len = state_dict['positional_embedding'].shape[0]
                model_len = text_encoder.positional_embedding.shape[0]
                if ckpt_len > model_len:
                    logger.info(f"âš ï¸ Truncating positional embedding from {ckpt_len} to {model_len}")
                    state_dict['positional_embedding'] = state_dict['positional_embedding'][:model_len, :]

            text_encoder.load_state_dict(state_dict, strict=False)

    elif args.text_encoder_type == 'glove':
        logger.info("Building GloVe Text Encoder...")
        if not hasattr(dataset_train, 'vocab'):
            raise AttributeError("Dataset needs 'vocab' attribute for GloVe mode.")
        
        text_encoder = GloveTextEncoder(
            vocab_list=dataset_train.vocab, 
            glove_path=args.glove_path
        )
    
    if text_encoder is not None:
        text_encoder.to(device)
    elif args.text_encoder_type != 'precomputed': 
        raise ValueError("Text Encoder failed to initialize.")

    # -----------------------------------------------------------
    # 4. æ„å»ºæ¨¡å‹
    # -----------------------------------------------------------
    logger.info("Building Model...")
    model = build_model(args)
    # å¦‚æœ build_model å†…éƒ¨æ²¡æœ‰èµ‹å€¼ text_encoderï¼Œè¿™é‡Œæ‰‹åŠ¨èµ‹ä¸€æ¬¡
    if hasattr(model, 'text_encoder') and model.text_encoder is None:
        model.text_encoder = text_encoder
    model.to(device)

    # -----------------------------------------------------------
    # 5. åŒ¹é…å™¨å’ŒæŸå¤±
    # -----------------------------------------------------------
    matcher = HungarianMatcher(cost_class=2, 
                               cost_span=5, 
                               cost_giou=2)
    
    # [ä¿®æ”¹ A] é‡æ„ Loss æƒé‡é…ç½®
    weight_dict = {
        # Labels: åˆ†ç±»ä»»åŠ¡é€šå¸¸æ”¶æ•›å¾ˆå¿«ï¼Œæƒé‡ä» 5.0 é™ä¸º 2.0ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        'loss_labels': 2.0, 
        
        # Span & GIoU: æ ¸å¿ƒå›å½’ä»»åŠ¡ï¼Œä¿æŒé«˜æƒé‡ï¼Œä¸»å¯¼æ¢¯åº¦
        'loss_span': 5.0, 
        'loss_giou': 2.0, 
        
        # Quality: ä¿æŒä¸å˜ï¼Œç”¨äºè¾…åŠ©æ’åº
        'loss_quality': 2.0, 

        # [å…³é”®ä¿®æ”¹] Saliency: ä» 4.0 é™è‡³ 0.4
        # ç†ç”±: Raw Loss çº¦ä¸º 1.1ï¼Œä¹˜ä»¥ 0.4 åä¸º 0.44ï¼Œæ—¢èƒ½æä¾›è¾…åŠ©ä¿¡æ¯ï¼Œåˆä¸ä¼šæ©ç›–å›å½’ä»»åŠ¡çš„æ¢¯åº¦
        'loss_saliency': 0.4, 

        # Contrastive: ä¿æŒè¾ƒä½æƒé‡
        'loss_contrastive': 0.2, 

        # RecFW: å¦‚æœä¸æ˜¯æ ¸å¿ƒä»»åŠ¡ï¼Œå»ºè®®è¿›ä¸€æ­¥é™ä½æˆ–ä¿æŒ 0.1
        'loss_recfw': 0.1 
    }

    if args.aux_loss:
        aux_weight_dict = {}
        for i in range(args.dec_layers - 1):
            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})
        weight_dict.update(aux_weight_dict)

    # æ³¨å†Œæ‰€æœ‰éœ€è¦çš„ loss
    losses = ['labels', 'spans', 'quality', 'recfw', 'saliency'] 
    
    
    criterion = SetCriterion(matcher, weight_dict, losses=losses, eos_coef=args.eos_coef)
    criterion.to(device)

    # -----------------------------------------------------------
    # 6. ä¼˜åŒ–å™¨
    # -----------------------------------------------------------
    param_dicts = [
        {"params": [p for n, p in model.named_parameters() if "text_encoder" not in n and p.requires_grad], "lr": args.lr},
    ]
    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)

    # [ä¿®æ”¹ C] ä½¿ç”¨ CosineAnnealingLR æ›¿æ¢ MultiStepLR
    # Cosine è°ƒåº¦å™¨åœ¨å¾®è°ƒåæœŸèƒ½æ›´å¹³æ»‘åœ°é™ä½å­¦ä¹ ç‡ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨å±€éƒ¨æå°å€¼é™„è¿‘ç¨³å®šä¸‹æ¥
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.epochs,      # è®¾ä¸ºæ€» Epoch æ•°
        eta_min=args.lr * 0.01  # æœ€å°å­¦ä¹ ç‡è®¾ä¸ºåˆå§‹ LR çš„ 1%
    )
    
    # å°† quality_proj å’Œ masked_token ç­‰æ–°å‚æ•°åŠ å…¥ä¼˜åŒ–å™¨
    #param_dicts = [
    #    {"params": [p for n, p in model.named_parameters() if "text_encoder" not in n and p.requires_grad], "lr": args.lr},
    #]
    #optimizer = torch.optim.AdamW(param_dicts, lr=args.lr, weight_decay=args.weight_decay)

    # å°† StepLR æ›¿æ¢ä¸º MultiStepLR
    # main.py
    #lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(
    #    optimizer,
    #    milestones=[60, 80],  
    #    gamma=0.5             
    #)
    #lr_scheduler = torch.optim.lr_scheduler.StepLR(
    #    optimizer,
    #    step_size=args.lr_drop, # 30
    #    gamma=0.1
    #)
    
    # ===========================================================
    # [æ–°å¢] Resume / Fine-tuning é€»è¾‘
    # ===========================================================
    if args.resume:
        if args.resume.startswith('https'):
            checkpoint = torch.hub.load_state_dict_from_url(args.resume, map_location='cpu', check_hash=True)
        else:
            logger.info(f"Loading checkpoint from {args.resume}")
            checkpoint = torch.load(args.resume, map_location='cpu') # å…ˆåŠ è½½åˆ° CPU

        # 1. åŠ è½½æ¨¡å‹æƒé‡
        model_dict = model.state_dict()
        pretrained_dict = checkpoint['model_state_dict']
        
        # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„é”® (ä»¥é˜²ä¸‡ä¸€ä½ ä¿®æ”¹äº†æ¨¡å‹ç»“æ„)
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
        
        # æ›´æ–°æƒé‡
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)
        
        logger.info(f"âœ… Loaded {len(pretrained_dict)}/{len(model_dict)} parameters from checkpoint.")

        # 2. [å…³é”®] å…³äº Optimizer å’Œ Epoch çš„å¤„ç†
        # æƒ…å†µ A: å¦‚æœæ˜¯ã€æ–­ç‚¹ç»­è®­ã€‘(æ¯”å¦‚è®­ç»ƒäº†ä¸€åŠå´©äº†)ï¼Œä½ éœ€è¦æ¢å¤ optimizer å’Œ start_epoch
        # æƒ…å†µ B: å¦‚æœæ˜¯ã€Fine-tuning / ç¬¬äºŒé˜¶æ®µã€‘(å¦‚ä½ ç°åœ¨çš„æƒ…å†µ)ï¼Œæˆ‘ä»¬é€šå¸¸åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œ
        #         ä½¿ç”¨æ–°çš„ LR å’Œæ–°çš„ Scheduler ä»å¤´å¼€å§‹ä¼˜åŒ–ï¼Œæ‰€ä»¥ä¸è¦åŠ è½½ optimizerã€‚
        
        # è¿™é‡Œæˆ‘å†™äº†ä¸€ä¸ªè‡ªåŠ¨åˆ¤æ–­ï¼šå¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šçš„ start_epoch > 0ï¼Œåˆ™è®¤ä¸ºæ˜¯æ–­ç‚¹ç»­è®­ï¼ŒåŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if args.start_epoch > 0 and 'optimizer_state_dict' in checkpoint and 'epoch' in checkpoint:
             logger.info(f"Resuming optimizer and scheduler states from epoch {checkpoint['epoch']}...")
             optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
             # å¦‚æœ scheduler ä¹Ÿåœ¨ checkpoint é‡Œï¼Œä¹Ÿå¯ä»¥åŠ è½½
             # lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict']) 
             args.start_epoch = checkpoint['epoch'] + 1
        else:
             logger.info("ğŸš€ Starting Fine-tuning: Resetting Optimizer and Epoch count.")
    # -----------------------------------------------------------
    # 7. è®­ç»ƒå¾ªç¯
    # -----------------------------------------------------------
    logger.info(f"Start training for {args.epochs} epochs.")
    
    best_r1_07 = 0.0 
    
   # [ä¿®æ”¹ 1] åˆå§‹åŒ–ä¸‰ä¸ªæœ€ä½³æŒ‡æ ‡å˜é‡
    best_r1_07 = 0.0 
    best_r1_05 = 0.0       # æ–°å¢
    best_combined = 0.0    # æ–°å¢
    
    for epoch in range(args.epochs):
        train_one_epoch(model, criterion, dataloader_train, optimizer, device, epoch)
        
        # éªŒè¯ä¸æœ€ä½³æ¨¡å‹ä¿å­˜
        if dataloader_val is not None:
            metrics = evaluate(model, dataloader_val, device)
            
            # è·å–å½“å‰æŒ‡æ ‡
            current_r1_05 = metrics.get('R1@0.5', 0)
            current_r1_07 = metrics.get('R1@0.7', 0)
            current_combined = current_r1_05 + current_r1_07

            # -----------------------------------------------------------
            # ç­–ç•¥ A: è®°å½• R1@0.7 æœ€é«˜çš„æ¨¡å‹ (ä¿æŒåŸæœ‰é€»è¾‘)
            # -----------------------------------------------------------
            if current_r1_07 > best_r1_07:
                best_r1_07 = current_r1_07
                # ä¿å­˜ä¸º checkpoint_best.pth ä»¥ä¿æŒå…¼å®¹ï¼Œæˆ–è€…æ”¹ä¸º checkpoint_best_r1_07.pth
                best_path = os.path.join(args.save_dir, "checkpoint_best_r1_07.pth")
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'metrics': metrics
                }, best_path)
                logger.info(f"â­ New Best R1@0.7 Model! Score: {best_r1_07:.2f}%")

            # -----------------------------------------------------------
            # ç­–ç•¥ B: è®°å½• R1@0.5 æœ€é«˜çš„æ¨¡å‹ [æ–°å¢]
            # -----------------------------------------------------------
            if current_r1_05 > best_r1_05:
                best_r1_05 = current_r1_05
                best_path_05 = os.path.join(args.save_dir, "checkpoint_best_r1_05.pth")
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'metrics': metrics
                }, best_path_05)
                logger.info(f"â­ New Best R1@0.5 Model! Score: {best_r1_05:.2f}%")

            # -----------------------------------------------------------
            # ç­–ç•¥ C: è®°å½• (R1@0.5 + R1@0.7) ç»¼åˆæœ€é«˜çš„æ¨¡å‹ [æ–°å¢]
            # -----------------------------------------------------------
            if current_combined > best_combined:
                best_combined = current_combined
                best_path_combined = os.path.join(args.save_dir, "checkpoint_best_combined.pth")
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'metrics': metrics
                }, best_path_combined)
                logger.info(f"â­ New Best Combined Model! Score: {best_combined:.2f} (R1@0.5={current_r1_05:.2f}, R1@0.7={current_r1_07:.2f})")

        # ä¿å­˜æœ€æ–°çš„ Checkpoint (è¦†ç›–å¼ï¼Œç”¨äºæ¢å¤è®­ç»ƒ)
        ckpt_path = os.path.join(args.save_dir, "checkpoint_last.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'args': args
        }, ckpt_path)

if __name__ == '__main__':
    parser = get_args_parser()
    if not any(action.dest == 'text_encoder_type' for action in parser._actions):
        parser.add_argument('--text_encoder_type', default='clip', choices=['clip', 'glove'], help='Type of text encoder')
    if not any(action.dest == 'glove_path' for action in parser._actions):
        parser.add_argument('--glove_path', default='', type=str, help='Path to glove vectors')
    if not any(action.dest == 'clip_weight_path' for action in parser._actions):
        parser.add_argument('--clip_weight_path', default='', type=str, help='Path to pretrained CLIP weights')
    
    args = parser.parse_args()
    
    # [å…³é”®ä¿®å¤] åœ¨ args å®šä¹‰åï¼Œmain æ‰§è¡Œå‰è®¾ç½®é»˜è®¤å‚æ•°
    if not hasattr(args, 'vocab_size'):
        args.vocab_size = 49408
    if not hasattr(args, 'rec_fw'):
        args.rec_fw = True
        
    main(args)